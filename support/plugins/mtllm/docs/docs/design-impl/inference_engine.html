<!DOCTYPE html><html><head><title>MTLLM API Documentation | MTLLM Inference Engine</title><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=0"><meta name="robots" content="index,follow"><meta name="theme-color" content="#212121"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><link href="https://fonts.googleapis.com/css?family=Hind:400,700&amp;display=swap" rel="stylesheet"><link href="https://fonts.googleapis.com/css?family=Source+Code+Pro:300,400&amp;display=swap" rel="stylesheet"><link href="https://fonts.googleapis.com/icon?family=Material+Icons%7CMaterial+Icons+Outlined&amp;display=swap" rel="stylesheet"><style>
      body, input, button {
        font-family: 'Hind', sans-serif;
      }

      code, .hljs {
        font-family: 'Source Code Pro', 'Courier New', Courier, monospace;
      }

      .icon-font {
        font-family: 'Material Icons';
        font-weight: normal;
        font-style: normal;
        font-size: 24px;  /* Preferred icon size */
        display: inline-block;
        line-height: 1;
        text-transform: none;
        letter-spacing: normal;
        word-wrap: normal;
        white-space: nowrap;
        direction: ltr;

        /* Support for all WebKit browsers. */
        -webkit-font-smoothing: antialiased;
        /* Support for Safari and Chrome. */
        text-rendering: optimizeLegibility;

        /* Support for Firefox. */
        -moz-osx-font-smoothing: grayscale;

        /* Support for IE. */
        font-feature-settings: 'liga';
      }

      .icon-font.outline {
        font-family: 'Material Icons Outlined';
      }
    </style><script src="https://cdn.jsdelivr.net/npm/mermaid@8.11.2/dist/mermaid.min.js"></script><script>mermaid.initialize({"startOnLoad":true});
                        window.addEventListener('navigation', () => { mermaid.init(); });
                        console.log('Mermaid Plugin Initialized');</script><link href="/mtllm/docs/assets/codedoc-styles.css" rel="stylesheet"><script async="" defer="" src="/mtllm/docs/assets/codedoc-bundle.js"></script></head><body><div class="header-0-0-9"><script async="" defer="" src="https://buttons.github.io/buttons.js"></script><a class="github-button" data-color-scheme="no-preference: light; light: light; dark: dark;" data-icon="false" data-show-count="true" data-size="false" href="https://github.com/Jaseci-Labs/mtllm/">Star</a><br><br></div><div id="-codedoc-container" class="container"><h1 id="mtllm-inference-engine" class="heading-0-0-15"><span class="anchor-0-0-16" data-ignore-text=""><span class="icon-font" data-ignore-text="" style="vertical-align: sub">link</span></span>MTLLM Inference Engine</h1><h2 id="1-overview" class="heading-0-0-15"><span class="anchor-0-0-16" data-ignore-text=""><span class="icon-font" data-ignore-text="" style="vertical-align: sub">link</span></span>1. Overview</h2><p>The MTLLM (Meaning-Typed Large Language Model) Inference Engine is a core component of the MTLLM framework. It is responsible for managing the interaction between the application, the semantic registry, and the underlying Large Language Model (LLM). The Inference Engine handles the process of constructing prompts, managing LLM interactions, processing outputs, and implementing error handling and self-correction mechanisms.</p><div class="mermaid">graph TD
    A[Jaclang Application] --&gt; B[Compilation]
    B --&gt; C[SemRegistry]
    C --&gt; D[Pickle File]
    A --&gt; E[Runtime]
    E --&gt; F[MTLLM Inference Engine]
    F --&gt; G[LLM Model]
    F --&gt; H[Tool Integration]
    D -.-&gt; F
    G --&gt; I[Output Processing]
    I --&gt; J[Error Handling]
    J --&gt;|Error| F
    J --&gt;|Success| K[Final Output]</div><h2 id="2-key-components" class="heading-0-0-15"><span class="anchor-0-0-16" data-ignore-text=""><span class="icon-font" data-ignore-text="" style="vertical-align: sub">link</span></span>2. Key Components</h2><p>The MTLLM Inference Engine consists of several key components:</p><ol><li>Prompt Constructor</li><li>LLM Interface</li><li>Output Processor</li><li>Error Handler</li><li>Tool Integrator</li></ol><h3 id="21-prompt-constructor" class="heading-0-0-15"><span class="anchor-0-0-16" data-ignore-text=""><span class="icon-font" data-ignore-text="" style="vertical-align: sub">link</span></span>2.1 Prompt Constructor</h3><p>The Prompt Constructor is responsible for building the input prompt for the LLM. It incorporates semantic information from the SemRegistry, user inputs, and contextual data to create a comprehensive and meaningful prompt.</p><p>Key features:</p><ul><li>Semantic enrichment using SemRegistry data</li><li>Dynamic prompt structure based on the chosen method (ReAct, Reason, CoT)</li><li>Integration of type information and constraints</li><li>Inclusion of available tools and their usage instructions</li></ul><p>Files involved:</p><ul><li><a href="mtllm/aott.py"><code>aott.py</code></a> # aott_raise, get_all_type_explanations</li><li><a href="mtllm/plugin.py"><code>plugin.py</code></a> # with_llm method</li><li><a href="mtllm/types.py"><code>types.py</code></a> # Information, InputInformation, OutputHint, Tool classes</li></ul><h3 id="22-llm-interface" class="heading-0-0-15"><span class="anchor-0-0-16" data-ignore-text=""><span class="icon-font" data-ignore-text="" style="vertical-align: sub">link</span></span>2.2 LLM Interface</h3><p>The LLM Interface manages the communication between the MTLLM framework and the underlying Large Language Model. It handles sending prompts to the LLM and receiving raw outputs.</p><p>Key features:</p><ul><li>Abstraction layer for different LLM providers</li><li>Handling of API communication and error management</li><li>Handling Multi-Modal Inputs if applicable</li></ul><p>Files involved:</p><ul><li><a href="mtllm/aott.py"><code>aott.py</code></a> # aott_raise</li><li><a href="mtllm/llms/base.py">'llms/base.py'</a> # BaseLLM class, <strong>call</strong>, <strong>infer</strong></li></ul><h3 id="23-output-processor" class="heading-0-0-15"><span class="anchor-0-0-16" data-ignore-text=""><span class="icon-font" data-ignore-text="" style="vertical-align: sub">link</span></span>2.3 Output Processor</h3><p>The Output Processor is responsible for parsing and validating the raw output from the LLM. It ensures that the output meets the expected format and type constraints.</p><p>Key features:</p><ul><li>Extraction of relevant information from LLM output</li><li>Type checking and format validation</li><li>Conversion of string representations to Python objects (when applicable)</li></ul><div class="mermaid">sequenceDiagram
    participant A as Application
    participant M as MTLLM Engine
    participant S as SemRegistry
    participant L as LLM Model
    participant T as Tools
    participant E as Evaluator
    A-&gt;&gt;M: Call by_llm()
    M-&gt;&gt;S: Fetch Semantic Info
    M-&gt;&gt;M: Construct Prompt
    M-&gt;&gt;L: Send Prompt
    L-&gt;&gt;M: Return Raw Output
    M-&gt;&gt;E: Evaluate Output
    alt Evaluation Successful
        E-&gt;&gt;M: Return Result
        M-&gt;&gt;A: Return Final Output
    else Evaluation Failed
        E-&gt;&gt;M: Return Error
        M-&gt;&gt;M: Construct Error Prompt
        M-&gt;&gt;L: Send Error Prompt
        L-&gt;&gt;M: Return Corrected Output
        M-&gt;&gt;E: Re-evaluate Output
    end
    opt Tool Usage Required
        M-&gt;&gt;T: Execute Tool
        T-&gt;&gt;M: Return Tool Result
        M-&gt;&gt;L: Include Tool Result in Prompt
    end</div><p>Files involved:</p><ul><li><a href="mtllm/aott.py"><code>aott.py</code></a> # aott_raise</li><li><a href="mtllm/llms/base.py"><code>llms/base.py</code></a> # BaseLLM class, BaseLLM.resolve_output, BaseLLM._extract_output, BaseLLM.to_object, BaseLLM._fix_output</li></ul><h3 id="24-error-handler" class="heading-0-0-15"><span class="anchor-0-0-16" data-ignore-text=""><span class="icon-font" data-ignore-text="" style="vertical-align: sub">link</span></span>2.4 Error Handler</h3><p>The Error Handler manages error detection, classification, and the self-correction process. It works closely with the Output Processor to identify issues and initiate corrective actions.</p><p>Key features:</p><ul><li>Error detection and classification</li><li>Generation of targeted feedback for the LLM</li><li>Management of the self-correction loop</li><li>Implementation of fallback strategies</li></ul><div class="mermaid">graph TD
    A[LLM Output] --&gt; B{Validate Output}
    B --&gt;|Valid| C[Return Result]
    B --&gt;|Invalid| D[Classify Error]
    D --&gt; E[Generate Error Feedback]
    E --&gt; F[Create Self-Correction Prompt]
    F --&gt; G[Submit to LLM]
    G --&gt; H{Check Retry Count}
    H --&gt;|Max Retries Reached| I[Return Error to Application]
    H --&gt;|Retries Available| B</div><p>Files involved:</p><ul><li><a href="mtllm/llms/base.py"><code>llms/base.py</code></a> # BaseLLM._check_output , BaseLLM._extract_output, BaseLLM.to_object, BaseLLM._fix_output</li></ul><h3 id="25-tool-integrator" class="heading-0-0-15"><span class="anchor-0-0-16" data-ignore-text=""><span class="icon-font" data-ignore-text="" style="vertical-align: sub">link</span></span>2.5 Tool Integrator</h3><p>The Tool Integrator manages the integration and execution of external tools within the inference process. It allows the LLM to leverage additional capabilities when needed.</p><p>Key features:</p><ul><li>Integration of tool results into the LLM prompt</li><li>Error handling for tool execution in ReAct mode</li></ul><div class="mermaid">sequenceDiagram
    participant A as Application
    participant M as MTLLM Engine
    participant L as LLM Model
    participant T as Tools
    A-&gt;&gt;M: Call by_llm()
    M-&gt;&gt;L: Send Prompt
    L-&gt;&gt;M: Return Tool Usage Request
    M-&gt;&gt;T: Execute Tool
    T-&gt;&gt;M: Return Tool Result
    M-&gt;&gt;L: Include Tool Result in Prompt
    L-&gt;&gt;M: Return Final Output
    M-&gt;&gt;A: Return Final Output</div><p>Files involved:</p><ul><li><a href="mtllm/plugin.py"><code>plugin.py</code></a> # callable_to_tool</li><li><a href="mtllm/types.py"><code>types.py</code></a> # Tool class</li><li><a href="mtllm/tools/base.py"><code>tools/base.py</code></a> # Tool class</li><li>[`tools/&lt;math_utils.py/serper.py/wikipedia.py&gt;](mtllm/tools) # Predefined tools</li></ul><h2 id="3-inference-process" class="heading-0-0-15"><span class="anchor-0-0-16" data-ignore-text=""><span class="icon-font" data-ignore-text="" style="vertical-align: sub">link</span></span>3. Inference Process</h2><p>The MTLLM Inference Engine follows a structured process for each inference request:</p><ol><li><p><strong>Initialization</strong>: The inference process begins when the <code>with_llm</code> function is called from the application.</p></li><li><p><strong>Semantic Information Retrieval</strong>: The engine queries the SemRegistry to retrieve relevant semantic information based on the current context and input parameters.</p></li><li><p><strong>Prompt Construction</strong>: The Prompt Constructor builds the initial prompt, incorporating semantic information, input data, and any relevant type constraints or tool descriptions.</p></li><li><p><strong>LLM Interaction</strong>: The constructed prompt is sent to the LLM via the LLM Interface. The raw output is received and passed to the Output Processor.</p></li><li><p><strong>Output Processing</strong>: The Output Processor parses the LLM's response, extracting the relevant information and performing initial validation.</p></li><li><p><strong>Error Checking</strong>: The processed output is checked for errors or inconsistencies. If issues are detected, the Error Handler is invoked to manage the self-correction process.</p></li><li><p><strong>Tool Execution (if required)</strong>: If the LLM's response indicates the need for tool usage, the Tool Integrator manages the execution of the required tool and integration of its results.</p></li><li><p><strong>Iteration (if necessary)</strong>: Steps 4-7 may be repeated if error correction or additional tool usage is required.</p></li><li><p><strong>Final Output</strong>: Once a valid output is obtained, it is returned to the calling application.</p></li></ol><div class="mermaid">sequenceDiagram
    participant App as Application
    participant IE as Inference Engine
    participant PC as Prompt Constructor
    participant SR as Semantic Registry
    participant LLM as LLM Interface
    participant OP as Output Processor
    participant EH as Error Handler
    participant TI as Tool Integrator
    App-&gt;&gt;IE: Call by_llm()
    IE-&gt;&gt;SR: Retrieve semantic info
    SR--&gt;&gt;IE: Return semantic info
    IE-&gt;&gt;PC: Construct prompt
    PC--&gt;&gt;IE: Return initial prompt
    loop Until valid output or max iterations
        IE-&gt;&gt;LLM: Send prompt
        LLM--&gt;&gt;IE: Return raw output
        IE-&gt;&gt;OP: Process output
        OP--&gt;&gt;IE: Return processed output
        IE-&gt;&gt;EH: Check for errors
        alt Error detected
            EH--&gt;&gt;IE: Return correction prompt
            IE-&gt;&gt;PC: Update prompt
        else Tool required
            IE-&gt;&gt;TI: Execute tool
            TI--&gt;&gt;IE: Return tool result
            IE-&gt;&gt;PC: Add tool result to prompt
        else Valid output
            IE-&gt;&gt;App: Return final output
        end
    end</div><h2 id="4-implementation-details" class="heading-0-0-15"><span class="anchor-0-0-16" data-ignore-text=""><span class="icon-font" data-ignore-text="" style="vertical-align: sub">link</span></span>4. Implementation Details</h2><h3 id="41-with_llm-function" class="heading-0-0-15"><span class="anchor-0-0-16" data-ignore-text=""><span class="icon-font" data-ignore-text="" style="vertical-align: sub">link</span></span>4.1 <code>with_llm</code> Function</h3><p>The <code>with_llm</code> function serves as the main entry point for the MTLLM Inference Engine. It orchestrates the entire inference process, initializing the necessary components, managing the flow of information, and handling the iterative process of obtaining a valid output from the LLM.</p><h3 id="42-error-handling-and-self-correction" class="heading-0-0-15"><span class="anchor-0-0-16" data-ignore-text=""><span class="icon-font" data-ignore-text="" style="vertical-align: sub">link</span></span>4.2 Error Handling and Self-Correction</h3><p>The Error Handler implements a sophisticated mechanism for detecting and correcting errors in the LLM's output. It maintains a count of correction attempts, generates targeted prompts for error correction, and determines when to terminate the correction process.</p><h3 id="43-tool-integration" class="heading-0-0-15"><span class="anchor-0-0-16" data-ignore-text=""><span class="icon-font" data-ignore-text="" style="vertical-align: sub">link</span></span>4.3 Tool Integration</h3><p>The Tool Integrator manages the execution of external tools and the integration of their results into the inference process. It converts normal functions to tools and executes them in the context of the inference engine.</p><h2 id="5-extensibility-and-customization" class="heading-0-0-15"><span class="anchor-0-0-16" data-ignore-text=""><span class="icon-font" data-ignore-text="" style="vertical-align: sub">link</span></span>5. Extensibility and Customization</h2><p>The MTLLM Inference Engine is designed with extensibility in mind. Key areas for customization include:</p><ol><li><strong>Prompting Strategies</strong>: New prompting methods can be added by extending the Model class or changing the MTLLM_PROMPTs</li><li><strong>LLM Providers</strong>: Support for new LLM providers can be added by implementing the BaseLLM interface.</li><li><strong>Tool Integration</strong>: New tools can be easily registered and integrated into the inference process.</li><li><strong>Error Handling</strong>: Custom error detection and correction strategies can be implemented by simple prompting changes.</li></ol><h2 id="6-performance-considerations" class="heading-0-0-15"><span class="anchor-0-0-16" data-ignore-text=""><span class="icon-font" data-ignore-text="" style="vertical-align: sub">link</span></span>6. Performance Considerations</h2><p>The MTLLM Inference Engine is designed to balance performance and flexibility. Key performance considerations include:</p><ol><li><strong>Caching</strong>: Implement caching mechanisms for frequently used prompts or intermediate results.</li><li><strong>Asynchronous Processing</strong>: Utilize asynchronous programming techniques for non-blocking I/O operations, especially in LLM interactions.</li><li><strong>Batching</strong>: Implement batching strategies for processing multiple inference requests efficiently.</li><li><strong>Resource Management</strong>: Carefully manage memory usage/ token usage, especially when dealing with large prompts or outputs.</li></ol><h2 id="7-security-considerations" class="heading-0-0-15"><span class="anchor-0-0-16" data-ignore-text=""><span class="icon-font" data-ignore-text="" style="vertical-align: sub">link</span></span>7. Security Considerations</h2><p>Security is a critical aspect of the MTLLM Inference Engine design:</p><ol><li><strong>Input Sanitization</strong>: Implement robust input sanitization to prevent injection attacks.</li><li><strong>Tool Execution Sandboxing</strong>: Execute external tools in a controlled environment to prevent unauthorized actions.</li><li><strong>Output Validation</strong>: Implement thorough output validation to ensure the LLM's responses don't contain harmful content.</li><li><strong>API Key Management</strong>: Securely manage and rotate API keys for LLM providers.</li></ol><h2 id="8-future-improvements" class="heading-0-0-15"><span class="anchor-0-0-16" data-ignore-text=""><span class="icon-font" data-ignore-text="" style="vertical-align: sub">link</span></span>8. Future Improvements</h2><p>Potential areas for future improvement of the MTLLM Inference Engine include:</p><ol><li><strong>Advanced Caching Strategies</strong>: Implement more sophisticated caching mechanisms to improve performance.</li><li><strong>Multi-Model Support</strong>: Enable the use of multiple LLMs within a single inference process for enhanced capabilities.</li><li><strong>Federated Learning Integration</strong>: Explore the integration of federated learning techniques for privacy-preserving model updates.</li><li><strong>Explainability Features</strong>: Develop features to provide explanations for the LLM's decision-making process.</li><li><strong>Adaptive Prompting</strong>: Implement adaptive prompting strategies that evolve based on the success rates of different prompt structures.</li></ol><p>This documentation provides a comprehensive overview of the MTLLM Inference Engine's design and implementation. It covers the key components, the inference process, implementation details, extensibility options, and important considerations for performance and security.</p><div class="contentnav-0-0-14" data-no-search=""><a href="#mtllm-inference-engine" class="h1" data-content-highlight="mtllm-inference-engine">MTLLM Inference Engine</a><a href="#1-overview" class="h2" data-content-highlight="1-overview">1. Overview</a><a href="#2-key-components" class="h2" data-content-highlight="2-key-components">2. Key Components</a><a href="#21-prompt-constructor" class="h3" data-content-highlight="21-prompt-constructor">2.1 Prompt Constructor</a><a href="#22-llm-interface" class="h3" data-content-highlight="22-llm-interface">2.2 LLM Interface</a><a href="#23-output-processor" class="h3" data-content-highlight="23-output-processor">2.3 Output Processor</a><a href="#24-error-handler" class="h3" data-content-highlight="24-error-handler">2.4 Error Handler</a><a href="#25-tool-integrator" class="h3" data-content-highlight="25-tool-integrator">2.5 Tool Integrator</a><a href="#3-inference-process" class="h2" data-content-highlight="3-inference-process">3. Inference Process</a><a href="#4-implementation-details" class="h2" data-content-highlight="4-implementation-details">4. Implementation Details</a><a href="#41-with_llm-function" class="h3" data-content-highlight="41-with_llm-function">4.1 with_llm Function</a><a href="#42-error-handling-and-self-correction" class="h3" data-content-highlight="42-error-handling-and-self-correction">4.2 Error Handling and Self-Correction</a><a href="#43-tool-integration" class="h3" data-content-highlight="43-tool-integration">4.3 Tool Integration</a><a href="#5-extensibility-and-customization" class="h2" data-content-highlight="5-extensibility-and-customization">5. Extensibility and Customization</a><a href="#6-performance-considerations" class="h2" data-content-highlight="6-performance-considerations">6. Performance Considerations</a><a href="#7-security-considerations" class="h2" data-content-highlight="7-security-considerations">7. Security Considerations</a><a href="#8-future-improvements" class="h2" data-content-highlight="8-future-improvements">8. Future Improvements</a></div></div><div id="-codedoc-toc" class="toc-0-0-11"><script>
     if (window.matchMedia('(min-width: 1200px)').matches) {
       if (!localStorage.getItem('-codedoc-toc-active')) {
         localStorage.setItem('-codedoc-toc-active', "true");
       }
     }
     </script><div class="content-0-0-12"><p><a href="/mtllm/">Home</a></p><div class="collapse-0-0-8 "><script id="nNaw_rYa_m">(function(){function load(){if (window.__sdh_transport){window.__sdh_transport("nNaw_rYa_m", "BR5Z0MA6Aj4P2zER2ZLlUg==", {});} }; if (document.readyState == 'complete') load(); else window.addEventListener('load', load); window.setImmediate = window.setImmediate || function(f){setTimeout(f, 0)}; })()</script><div class="label" onclick="this.parentElement.classList.toggle('open')"><span class="text">Quick Start</span><span class="icon-font closed">chevron_right</span></div><div class="content"><p><a href="/mtllm/docs/quickstart/installation">Installation</a>
<a href="/mtllm/docs/quickstart/minimal-working-example">Minimal Working Example</a></p></div></div><div class="collapse-0-0-8 "><script id="CtOAkaanaG">(function(){function load(){if (window.__sdh_transport){window.__sdh_transport("CtOAkaanaG", "BR5Z0MA6Aj4P2zER2ZLlUg==", {});} }; if (document.readyState == 'complete') load(); else window.addEventListener('load', load); window.setImmediate = window.setImmediate || function(f){setTimeout(f, 0)}; })()</script><div class="label" onclick="this.parentElement.classList.toggle('open')"><span class="text">Design and Implementation</span><span class="icon-font closed">chevron_right</span></div><div class="content"><p><a href="/mtllm/docs/design-impl/sem_registry">SemRegistry</a>
<a href="/mtllm/docs/design-impl/inference_engine">Inference Engine</a></p></div></div><div class="collapse-0-0-8 "><script id="ubUHgfCrxj">(function(){function load(){if (window.__sdh_transport){window.__sdh_transport("ubUHgfCrxj", "BR5Z0MA6Aj4P2zER2ZLlUg==", {});} }; if (document.readyState == 'complete') load(); else window.addEventListener('load', load); window.setImmediate = window.setImmediate || function(f){setTimeout(f, 0)}; })()</script><div class="label" onclick="this.parentElement.classList.toggle('open')"><span class="text">Building Blocks</span><span class="icon-font closed">chevron_right</span></div><div class="content"><p><a href="/mtllm/docs/building-blocks/language_models">Language Models</a>
<a href="/mtllm/docs/building-blocks/semstrings">Semstrings</a>
<a href="/mtllm/docs/building-blocks/functions_methods">Functions and Methods</a>
<a href="/mtllm/docs/building-blocks/object_init">Object Initialization</a>
<a href="/mtllm/docs/building-blocks/multimodality">Multimodality</a></p></div></div><div class="collapse-0-0-8 "><script id="mNGIUJViqh">(function(){function load(){if (window.__sdh_transport){window.__sdh_transport("mNGIUJViqh", "BR5Z0MA6Aj4P2zER2ZLlUg==", {});} }; if (document.readyState == 'complete') load(); else window.addEventListener('load', load); window.setImmediate = window.setImmediate || function(f){setTimeout(f, 0)}; })()</script><div class="label" onclick="this.parentElement.classList.toggle('open')"><span class="text">Tutorials</span><span class="icon-font closed">chevron_right</span></div><div class="content"><p><a href="/mtllm/docs/tutorials/rpg_game">RPG Game Level Generation</a></p></div></div><div class="collapse-0-0-8 "><script id="KUsEXqZbGx">(function(){function load(){if (window.__sdh_transport){window.__sdh_transport("KUsEXqZbGx", "BR5Z0MA6Aj4P2zER2ZLlUg==", {});} }; if (document.readyState == 'complete') load(); else window.addEventListener('load', load); window.setImmediate = window.setImmediate || function(f){setTimeout(f, 0)}; })()</script><div class="label" onclick="this.parentElement.classList.toggle('open')"><span class="text">API Reference</span><span class="icon-font closed">chevron_right</span></div><div class="content"><p><a href="/mtllm/docs/api/mtllm">MTLLM</a></p></div></div><div class="collapse-0-0-8 "><script id="MuRACoEtvz">(function(){function load(){if (window.__sdh_transport){window.__sdh_transport("MuRACoEtvz", "BR5Z0MA6Aj4P2zER2ZLlUg==", {});} }; if (document.readyState == 'complete') load(); else window.addEventListener('load', load); window.setImmediate = window.setImmediate || function(f){setTimeout(f, 0)}; })()</script><div class="label" onclick="this.parentElement.classList.toggle('open')"><span class="text">Tips and Tricks</span><span class="icon-font closed">chevron_right</span></div><div class="content"><p><a href="/mtllm/docs/tips-n-tricks/existing_application">Using MTLLM in your existing Application</a>
<a href="/mtllm/docs/tips-n-tricks/when_to_use_semstrings">When to use Semstrings</a>
<a href="/mtllm/docs/tips-n-tricks/create_own_lm">Create your own Language Model</a></p></div></div><p><a href="/mtllm/docs/faqs">FAQs</a></p></div><div class="search-0-0-13"><script id="LGNJhVsPgi">(function(){function load(){if (window.__sdh_transport){window.__sdh_transport("LGNJhVsPgi", "CEp7LAl0nnWrqHIN8Qnt6g==", {"repo":"mtllm","user":"Jaseci-Labs","root":"docs/md","pick":"\\.md$","drop":"(^_)|(\\/_)"});} }; if (document.readyState == 'complete') load(); else window.addEventListener('load', load); window.setImmediate = window.setImmediate || function(f){setTimeout(f, 0)}; })()</script></div></div><div class="footer-0-0-10"><div class="left"><script id="SIVqtvPvsX">(function(){function load(){if (window.__sdh_transport){window.__sdh_transport("SIVqtvPvsX", "KKHOIeoEcuIIR8G+qI09PQ==", {});} }; if (document.readyState == 'complete') load(); else window.addEventListener('load', load); window.setImmediate = window.setImmediate || function(f){setTimeout(f, 0)}; })()</script></div><div class="main"><div class="inside"><a href="https://github.com/Jaseci-Labs/mtllm/" target="_blank">GitHub</a></div></div><div class="right"><script id="pukxBTvxRf">(function(){function load(){if (window.__sdh_transport){window.__sdh_transport("pukxBTvxRf", "Xodqq8f8LP13F67p+cusew==", {});} }; if (document.readyState == 'complete') load(); else window.addEventListener('load', load); window.setImmediate = window.setImmediate || function(f){setTimeout(f, 0)}; })()</script></div></div><script id="SdUOKfJhEs">(function(){function load(){if (window.__sdh_transport){window.__sdh_transport("SdUOKfJhEs", "3GUK3xGbIE9fCSzaoTX0bA==", {"namespace":"/mtllm"});} }; if (document.readyState == 'complete') load(); else window.addEventListener('load', load); window.setImmediate = window.setImmediate || function(f){setTimeout(f, 0)}; })()</script></body></html>