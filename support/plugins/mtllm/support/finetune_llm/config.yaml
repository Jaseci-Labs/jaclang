model:
  hf_dataset: ["chandralegend/map_gen_randomized", "chandralegend/mtllm-level-gen-test"]
  hf_model: "HuggingFaceTB/SmolLM-1.7B-Instruct"
  output_model: "mtllm-levelgen-smollm-1.7b-chat"

lora_config:
  r: 8
  lora_alpha: 8
  lora_dropout: 0.05
  bias: "none"
  task_type: "CASUAL_LM"

training_args:
  learning_rate: 0.00002
  lr_scheduler_type: "cosine"
  per_device_train_batch_size: 1
  # per_device_eval_batch_size: 2
  gradient_accumulation_steps: 4
  optim: "paged_adamw_32bit"
  save_strategy: "steps"
  save_steps: 100
  # eval_strategy: "steps"
  # eval_steps: 500
  logging_steps: 50
  save_total_limit: 4
  max_steps: -1
  fp16: false
  bf16: false
  # eval_on_start: true
  # do_eval: true

trainer:
  dataset_text_field: "text"
  max_seq_length: 2048

push_to_hf:
  hf_username: chandralegend